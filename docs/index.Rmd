---
title: 'ANÁLISE BIBLIOMÉTRICA DE GESTÃO ESPORTIVA'
subtitle: 'Pesquisa em Finanças'
author: '_Autores:_ Prof. Marcelo Fonseca, Wilson Nakamura, Giancarlo, José Marcos, Victor Valerio'
date: '_Última atualização:_ data'
output:
  html_document
---

<div style="text-align: justify">

```{r , results = 'hide', echo = F, warning = F, message = F, error = F}


library(magrittr)


library(bibliometrix)


library(tidyverse)


# Importação dos dados da base Web Of Science:
dados_wos <- bibliometrix::convert2df(file = list.files(path = '../dados/arquivos_wos_12_04_21', full.names = TRUE),
                                      dbsource = 'wos',
                                      format = 'bibtex') %>% 

  
  dplyr::filter(DT == 'ARTICLE',
                PY <= 2020,
                LA == 'ENGLISH',
                !stringr::str_detect(ID,'RUGBY'),
                !stringr::str_detect(ID,'MEDICINE'),
                !stringr::str_detect(ID,'CONCUSSION'),
                !stringr::str_detect(ID,'EPIDEMIOLOGY'),
                !stringr::str_detect(ID,'INJURY'),
                !stringr::str_detect(ID,'GPS'),
                !stringr::str_detect(ID,'KNEE'),
                !stringr::str_detect(DE,'RUGBY'),
                !stringr::str_detect(DE,'CONCUSSION'),
                !stringr::str_detect(DE,'EPIDEMIOLOGY'),
                !stringr::str_detect(DE,'INJURY'),
                !stringr::str_detect(DE,'GPS'),
                !stringr::str_detect(DE,'KNEE'),
                !stringr::str_detect(DE,'MEDICINE'))
                
               
# Importação dos dados da base scopus:
dados_scopus <- bibliometrix::convert2df(file = list.files(path = '../dados/arquivos_scopus_12_04_21', full.names = TRUE),
                                         dbsource = 'scopus',
                                         format = 'bibtex') %>% 

  dplyr::filter(DT == 'ARTICLE',
                PY <= 2020,
                LA == 'ENGLISH',
                !stringr::str_detect(ID,'RUGBY'),
                !stringr::str_detect(ID,'MEDICINE'),
                !stringr::str_detect(ID,'CONCUSSION'),
                !stringr::str_detect(ID,'EPIDEMIOLOGY'),
                !stringr::str_detect(ID,'INJURY'),
                !stringr::str_detect(ID,'GPS'),
                !stringr::str_detect(ID,'KNEE'),
                !stringr::str_detect(DE,'RUGBY'),
                !stringr::str_detect(DE,'CONCUSSION'),
                !stringr::str_detect(DE,'EPIDEMIOLOGY'),
                !stringr::str_detect(DE,'INJURY'),
                !stringr::str_detect(DE,'GPS'),
                !stringr::str_detect(DE,'KNEE'),
                !stringr::str_detect(DE,'MEDICINE'))


dados_scopus$ID <- stringr::str_replace_all(dados_scopus$ID,'SPORTS','SPORT')
dados_scopus$ID <- stringr::str_replace_all(dados_scopus$ID,'HUMANS','HUMAN')


dados_wos$ID <- stringr::str_replace_all(dados_wos$ID,'SPORTS','SPORT')
dados_wos$ID <- stringr::str_replace_all(dados_wos$ID,'HUMANS','HUMAN')
            

# Juntando os resultados das suas bases e removendo os documentos duplicados:
M <- bibliometrix::mergeDbSources(dados_wos, dados_scopus, remove.duplicated = TRUE) 


# Calculando a base dos indicadores bibliométricos:
results <- bibliometrix::biblioAnalysis(M)


sumario <- summary(results, k = 10, pause = F)


LCdata <- bibliometrix::localCitations(M, fast.search = FALSE, sep = ';')


histResults <- bibliometrix::histNetwork(M, min.citations = 10, sep = ";")


```

# 1 INTRODUCTION 

</br>

- **Relevância do futebol em relação aos demais esportes do mundo**; 

</br>

- **Alto volume de dinheiro**;

</br>

- **Tendência de uma gerência profissional, denominado de clube empresa**;

</br>

- **Problemas financeiros e assimetria de resultados**;

</br>

- **Objetivos (com base nisso é importante analisar o estado da arte de análise financiera no futebol)**.

</br>

# 2 LITERATURE REVIEW  

</br>

- **Buscar na base de dados os artigos que já realizaram revisão de literatura e comentar os principais
resultados de cada um.**

</br>

# 3 METHOD AND DATA

## 3.1 Prisma with Bibliometric Aproach

OBS: sequência de análise da parte bibliométrica:

</br>

- **Autor** 

</br>

- **Artigo** 

</br>

- **Jornal**

</br>

- **Instituições**

</br>

- **Países** 

</br>

- **Tema**



## 3.3 Data


</br>

O presente trabalho foi organizado em uma sequência de três passos, Data
Import, Data Transformation e Data Modelling and Mapping conforme
apresentado na Figura 3.1:

</br>

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 3.1 – Flow chart showing data collection, modelling and mapping'}


# Incluindo a Figura do método
knitr::include_graphics('../imagens/figura_metodo.png')


```






A etapa de importação de dados consiste em gerar arquivos nas bases de
dados cientifícos Scopus e Web of Science com metadados informações de
citação, bibliográficas e de referência, palavras-chave, resumo e
detalhes de financiamento a respeito dos artigos de educação financeira.

Sobre este aspecto, cumpre destacar que em ambas as bases de dados foi
utilizado como único descritor a palavra-chave "Financial Literacy"
buscado nos tópicos dos artigos (título, resumo, authors keywords e
keywords plus) dos artigos. Limitou-se a amostra a apenas artigos de
língua inglesa publicados até o ano de 2020, sendo que, os dois
primeiros filtros são necessários na medida em que padronizam o estilo
de referências e também de codificação, enquanto que, o filtro de
período foi aplicado para que a análise seja aplicada apenas para os
anos completos. Nesta etapa foram encontrados **XXXX** documentos e
**XXXX** nas bases de dados Scopus e Web of Science, respectivamente.

Por sua vez, a etapa de tranformação dos dados tem por finalidade
transformar os metadados importados uma única única planilha consolidada
com todas as informações de ambas as bases de dados, excluindo-se as
informações duplicadas

Após aplicação dos filtros, foram organizados os metadados de 1479 e
1814 artigos de língua inglesa sobre educação financeira até o ano de
2020 nas bases de dados Scopus e Web of Science, respectivamente.

Ao aplicar as etapas de análise descritas acima, pode-se obter a coleção
de artigos a serem analisados. Neste sentido, as principais informações
a respeito da coleção de artigos é apresentada na Tabela 1.1, abaixo:

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Table 3.1 – Main information about data'}


main <- tibble::as_tibble(sumario[["MainInformationDF"]]) %>% 
  dplyr::filter(!Description %in% c('MAIN INFORMATION ABOUT DATA',
                                    'DOCUMENT TYPES',
                                    'article',
                                    'DOCUMENT CONTENTS',
                                    'AUTHORS',
                                    'Author Appearances',
                                    'AUTHORS COLLABORATION',
                                    'Co-Authors per Documents'))


knitr::kable(main)


```

# Results

## Themes


```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 4.1 – Word growth'}


worddynamics <- KeywordGrowth(M, 
                              Tag = 'ID', 
                              sep = ";", 
                              top = 10, 
                              cdf = TRUE)


colnames(worddynamics) <- stringr::str_to_lower(colnames(worddynamics))


worddynamics %>% 
  pivot_longer(!(year), names_to = 'Words', values_to = 'Ndy') %>% 
  ggplot(mapping = aes(x = year, y = Ndy, color = Words)) + 
  labs(color = '') +
  geom_line(size = 0.5) +
  geom_point(size = 1.5) +
  ylab('Annual Occurrences') +
  theme(legend.position = 'bottom')


```



```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 4.2 – Thematic Map'}


Map <- thematicMap(M,
                   field = 'ID', 
                   n = 250, 
                   minfreq = 5,
                   stemming = FALSE, 
                   size = 0.5,
                   n.labels=3, repel = TRUE) 


plot(Map$map)


```

Como fazer a análise: • Os temas no quadrante superior direito são bem
desenvolvidos e importantes para a estruturação de um campo de pesquisa.
Eles são conhecidos como temas motores da especialidade, pois apresentam
forte centralidade e alta densidade. A colocação de temas nesse
quadrante implica que eles estejam relacionados externamente a conceitos
aplicáveis a outros temas que estão conceitualmente intimamente
relacionados.

• Os temas no quadrante superior esquerdo desenvolveram laços internos
bem desenvolvidos, mas laços externos sem importância e, portanto, são
de importância apenas marginal para o campo. Esses temas são muito
especializados e de caráter periférico.

• Os temas no quadrante inferior esquerdo são pouco desenvolvidos e
marginais. Os temas desse quadrante têm baixa densidade e baixa
centralidade, representando principalmente temas emergentes ou
desaparecendo.

• Os temas no quadrante inferior direito são importantes para um campo
de pesquisa, mas não são desenvolvidos. Portanto, esse quadrante agrupa
temas transversais e gerais, básicos.


## Authors Analysis

ANÁLISE DE PRODUTIVIDADE

Em primeiro lugar, para se ter uma visão geral da produtividades dos
autores neste campo de estudo testou-se o ajuste a lei de lotka.

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 4.3 – Scientific Productivity'}

L <- lotka(results)

Lobserved <- as_tibble(L[["AuthorProd"]])

colnames(Lobserved) <- c('Articles', 'Authors', 'Observed')

Theoretical <- as_tibble(10^(log10(L$C)-2*log10(L$AuthorProd[,1])))

colnames(Theoretical) <- c('Theoretical')

lotkatest <- cbind(Lobserved, Theoretical)

lotkatest$Observed <- as.numeric(lotkatest$Observed)
  
lotkatest$Theoretical <- as.numeric(lotkatest$Theoretical)

lotkatest <- lotkatest %>% 
  gather(Observed,
         Theoretical,
         key = 'Type',
         value = 'Value')

ggplot(lotkatest, mapping = aes(x = Articles, y = Value, color = Type)) + 
  geom_line(size = 0.5) +
  geom_point(size = 1.5) +
  labs(color = '') + 
  ylab('Freq. of Authors') + 
  xlab('Articles') +
  theme(legend.position = 'bottom')


# Beta coefficient estimate
#L$Beta
# Constant
#L$C
# Goodness of fit
#L$R2
# P-value of K-S two sample test
#L$p.value


```

A partir da aplicação do teste de duas amostras de Kolmogorov-Smirnoff
calculou-se um p-valor da ordem de 0,15, atestando que não há diferença
significativa entre as distribuições de Lotka observada e teórica.

Já na Tabela a seguir, pode-se observar o impacto para a literatura dos
autores mais produtivos a partir dos indicadores...

ANÁLISE DE IMPACTO

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Table 4.1 – Authors Impact'}

LCauthor <- as_tibble(LCdata[['Authors']])
colnames(LCauthor) <- c('Authors','Local Citations') 
authors <- gsub(","," ",names(results$Authors)[1:10])


indices <- Hindex(M, 
                  field = "author", 
                  elements = authors, 
                  sep = ";", 
                  years = 50)
indices <- as_tibble(indices[["H"]])
colnames(indices) <- c('Authors',
                       'H-Index',
                       'G_Index',
                       'M_Index',
                       '1',
                       'Documents',
                       '2')
indices <- indices %>% 
  select(Authors,
         `H-Index`,
         Documents)


AuthorsFrac <- as_tibble(results[["AuthorsFrac"]])
colnames(AuthorsFrac) <- c('Authors','Documents Fractionalized') 
tabauthors <- full_join(indices, AuthorsFrac, 'Authors')
tabauthors <- full_join(tabauthors, LCauthor, 'Authors')
tabauthors <- na.omit(tabauthors)
tabauthors$`Documents Fractionalized` <- round(as.numeric(tabauthors$`Documents Fractionalized`),2)
tabauthors$Authors <- stringr::str_to_title(tabauthors$Authors)
tabauthors <- tabauthors %>% 
  arrange(desc(`H-Index`))


knitr::kable(tabauthors, align = c('c','c','c','c','c'))


```

Realizar avaliações gerais.

ANÁLISE DE COLABORAÇÃO

```{r , results = 'hide', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 4.6 – Authors Collaboration'}


collaboration_authors <- biblioNetwork(M, 
                                      analysis = 'coupling', 
                                      network = 'authors',
                                      sep = ';')


networkPlot(collaboration_authors,
            Title = '',              # Não adicionar título
            n = 20,                  # Quantidade de nós na rede
            size = 15,               # Tamanho base dos nós
            size.cex = TRUE,         # Tamanho maior proporcional a grau
            type = 'auto',           # Layout da rede
            label.color = TRUE,      # Não está funcionando!???!
            remove.multiple = FALSE, # Mantém as diversas ligações
            edges.min = 5,           # Número mínimo de licações que devem aparecer
            edgesize = 5,            # Tamanho das ligações
            label.cex = TRUE,        # Nome proporcional ao grau
            curved = TRUE,           # Ligações curvas
            alpha = 0.75)            # Sombreamento das cores



```

sadasdasd








## Documents Analysis

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Table 4.2 – Most Cited Documents'}

LPapers <- as_tibble(LCdata[['Papers']])

LPapers <- LPapers[1:10,]

colnames(LPapers) <- c('Papers','DOI', 'Year', 'Local Citations', 'Global Citations') 

knitr::kable(LPapers)


```

Mapa historigrafico:

```{r , results = 'hide', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 4.7 – Historiograph'}


net <- histPlot(histResults, n = 20, size = 10, labelsize = 5,)


```

## Sources Analysis

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 4.8 – Sources Dynamics'}


M1 <- M %>% 
  select(SO, PY) %>% 
  mutate(Documents = 1) %>% 
  group_by(SO, PY) %>% 
  mutate(Soma = sum(Documents))

M1 <- M1[!duplicated(M1),]

M1 <- M1 %>% 
  select(SO, PY, Soma)

colnames(M1) <- c('Sources','Years','Documents')

sources <- as_tibble(results[['Sources']])

sources <- sources[1:10,]

colnames(sources) <- c('Sources', 'n')

M1 <- full_join(M1, sources, 'Sources')

M1 <- na.omit(M1)

M1 <- M1 %>% 
  group_by(Sources) %>% 
  arrange(Years) %>% 
  mutate(Documents = cumsum(Documents))

M1$Documents <- as.numeric(M1$Documents)

M1$Years <- lubridate::ymd(M1$Years, truncated = 2L)

ggplot(M1, mapping = aes(x = Years, y = Documents, color = Sources)) + 
  geom_line() +
  xlab('Years') + 
  ylab('Documents (cumulative sum) ')



```

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Table 4.3 – Sources Impact'}


sources <- as_tibble(results[['Sources']])

sources <- sources[1:10,]

indices <- Hindex(M, field = 'source', elements = sources$SO, sep = ';', years = 50)

indices <- as_tibble(indices[["H"]])

colnames(indices) <- c('Sources',
                       'H_Index',
                       'G_Index',
                       'M_Index',
                       '1',
                       'Documents',
                       '2')

indices <- indices %>% 
  select(Sources,
         H_Index,
         G_Index,
         M_Index,
         Documents)

indices$M_Index <- round(as.numeric(indices$M_Index),2)

knitr::kable(indices)


```

## Affiliation Analysis

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Table 4.4 – Affiliations Productivity'}


Affiliations <- as_tibble(results[["Affiliations"]])

Affiliations <- Affiliations[1:10,]

colnames(Affiliations) <- c('Affiliations','Documents')

knitr::kable(Affiliations)

```

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 4.9 – Affiliations Collaboration Network'}


NetMatrix <- biblioNetwork(M, analysis = "collaboration",  network = "universities", sep = ";")

net=networkPlot(NetMatrix,  normalize = "salton", weighted = NULL, n = 100, 
                Title = '', 
                type = 'fruchterman', 
                size = 10,
                size.cex = T,
                remove.multiple = T,
                labelsize = 1,
                label.n = 10,
                label.cex = F)

```

## Countries Analysis

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Table 4.5 – Countries Productivity'}


Countries <- as_tibble(results[['Countries']])

Countries <- Countries[1:10,]

colnames(Countries) <- c('Countries','Documents')

knitr::kable(Countries,
             
             escape = FALSE,
    
             format = 'html',
             
             align = c('c', 'l', 'l')) 


```



```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 4.10 – Countries Collaboration'}


CountriesCo <- as_tibble(results[["CountryCollaboration"]])

CountriesCo <- CountriesCo[1:10,]

CountriesCo <- CountriesCo %>% 
  mutate(Ratio = round((MCP/(SCP+MCP)),2))

colnames(CountriesCo) <- c('Countries','SCP', 'MCP', 'Ratio')

CountriesCo <- CountriesCo %>% 
  gather(SCP,
         MCP,
         key = 'Collaboration',
         value = 'Value')

CountriesCo$Value <- as.numeric(CountriesCo$Value)
CountriesCo <- CountriesCo %>% 
  
  filter(Collaboration!="SCP") %>% 
  
  mutate(Countries = fct_reorder(Countries, Ratio))




ggplot(CountriesCo, mapping = aes(y = Countries, x = Ratio)) +
  geom_col() + 
  ylab('Ratio of Collaboration') 


```

```{r , results = 'asis', echo = F, warning = F, message = F, error = F, fig.align ='center', out.width='100%', fig.cap = 'Figure 4.3 – Countries collaboration Network'}


M <- metaTagExtraction(M, Field = "AU_CO", sep = ";")

NetMatrix <- biblioNetwork(M, analysis = "collaboration",  network = "countries", sep = ";")

net=networkPlot(NetMatrix,  normalize = "salton", weighted = NULL, n = 100, 
                Title = '', 
                type = 'fruchterman', 
                size = 10,
                size.cex = T,
                remove.multiple = T,
                labelsize = 1,
                label.n = 10,
                label.cex = F)


```

# CONCLUSÕES
